{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.facebook.com/marketplace/calgary/vehicles?minPrice=1235&maxPrice=16000&maxMileage=180000&maxYear=2014&minMileage=40000&minYear=2008&make=audi&model=a4&sortBy=creation_time_descend&exact=false)'"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Startup\n",
    "\n",
    "# FBMP Scraper\n",
    "from splinter import Browser\n",
    "from bs4 import BeautifulSoup as soup\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from datetime import date\n",
    "\n",
    "# Discord\n",
    "import os\n",
    "import discord\n",
    "\n",
    "# Set up search parameters ***leave as '' for no option***\n",
    "csv_file_path = r'D:\\iso\\xp\\Shared Folder\\cars.csv'\n",
    "city = \"calgary\"\n",
    "base_url = f\"https://www.facebook.com/marketplace/{city}/vehicles?\"\n",
    "min_price = 1235\n",
    "max_price = 16000\n",
    "min_mileage = 40000\n",
    "max_mileage = 180000\n",
    "min_year = 2008\n",
    "max_year = 2014\n",
    "transmission = '' # 'automatic' or 'manual'\n",
    "make = 'audi' \n",
    "model = 'a4'\n",
    "\n",
    "# Set up full url\n",
    "url = f\"{base_url}minPrice={min_price}&maxPrice={max_price}&maxMileage={max_mileage}&maxYear={max_year}&minMileage={min_mileage}&minYear={min_year}\" \n",
    "if transmission != '':\n",
    "    url = url + f'&transmissionType={transmission}'\n",
    "if make != '':\n",
    "    url = url + f'&make={make}'\n",
    "    if model != '':\n",
    "        url = url + f'&model={model}'\n",
    "url = url + '&sortBy=creation_time_descend&exact=false)'\n",
    "\n",
    "# Create function\n",
    "def skim():\n",
    "    browser = Browser('chrome') # Set up Splinter\n",
    "    browser.visit(url) # Visit the website\n",
    "    \n",
    "    # Scroll down to load more results\n",
    "    scroll_count = 0 # ***Define the number of times to scroll the page***\n",
    "    scroll_delay = 2\n",
    "    for _ in range(scroll_count): \n",
    "        browser.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\") # Execute JavaScript to scroll to the bottom of the page\n",
    "        time.sleep(scroll_delay) # Pause for a moment to allow the content to load\n",
    "    html = browser.html\n",
    "    market_soup = soup(html, 'html.parser')\n",
    "    # Check if HTML was scraped correctly (uncomment below):\n",
    "    # print(market_soup)\n",
    "    time.sleep(4)\n",
    "    browser.quit()\n",
    "    # Extract all the necessary info and insert into lists\n",
    "    titles_div = market_soup.find_all('span', class_=\"x1lliihq x6ikm8r x10wlt62 x1n2onr6\")\n",
    "    titles_list = [title.text.strip() for title in titles_div]\n",
    "    prices_div = market_soup.find_all('span', class_=\"x193iq5w xeuugli x13faqbe x1vvkbs x1xmvt09 x1lliihq x1s928wv xhkezso x1gmr53x x1cpjm7i x1fgarty x1943h6x xudqn12 x676frb x1lkfr7t x1lbecb7 x1s688f xzsf02u\")\n",
    "    prices_list = [price.text.strip() for price in prices_div]\n",
    "    mileage_div = market_soup.find_all('span', class_=\"x193iq5w xeuugli x13faqbe x1vvkbs x1xmvt09 x1lliihq x1s928wv xhkezso x1gmr53x x1cpjm7i x1fgarty x1943h6x x4zkp8e x3x7a5m x1nxh6w3 x1sibtaa xo1l8bm xi81zsa\")\n",
    "    mileage_list = [mileage.text.strip() for mileage in mileage_div]\n",
    "    urls_div = market_soup.find_all('a', class_=\"x1i10hfl xjbqb8w x1ejq31n xd10rxx x1sy0etr x17r0tee x972fbf xcfux6l x1qhh985 xm0m39n x9f619 x1ypdohk xt0psk2 xe8uvvx xdj266r x11i5rnm xat24cr x1mh8g0r xexx8yu x4uap5 x18d9i69 xkhd6sd x16tdsg8 x1hl2dhg xggy1nq x1a2a7pz x1heor9g x1lku1pv\")\n",
    "    urls_list = [url.get('href') for url in urls_div]\n",
    "    # Make sure the lengths all match\n",
    "    print(len(titles_list))\n",
    "    print(len(prices_list))\n",
    "    print(len(mileage_list) / 2)\n",
    "    print(len(urls_list))\n",
    "    \n",
    "    # Data formatting\n",
    "    pattern = re.compile(r'(\\w+(?:-\\w+)?, [A-Z]{2})') # Create a regular expression pattern to match city and state entries like \"City, State\"\n",
    "    mileage_list2 = []\n",
    "    for item in mileage_list: # Iterate through the original mileage entries\n",
    "        mileage_list2.append(item)\n",
    "        if pattern.match(item) and len(mileage_list2) >= 2 and pattern.match(mileage_list2[-2]): # Check if the current mileage entry matches the pattern and there are at least two entries in the adjusted list\n",
    "            mileage_list2.insert(-1, '0K km') # If the conditions are met, insert \"0K km\" in between the two consecutive city and state entries\n",
    "    # Extract mileage from mileage list\n",
    "    # Define regular expressions to extract numeric mileage values in \"K km\" and \"K miles\" format\n",
    "    mileage_pattern_km = r'(\\d+)K km'\n",
    "    mileage_pattern_miles = r'(\\d+)K miles'\n",
    "    mileage_clean = []\n",
    "    for item in mileage_list2:\n",
    "        match_mileage_km = re.search(mileage_pattern_km, item) # Try to find a match for the \"K km\" format\n",
    "        match_mileage_miles = re.search(mileage_pattern_miles, item) # Try to find a match for the \"K miles\" format\n",
    "        if match_mileage_km or match_mileage_miles: # Check if either of the formats is found\n",
    "            if match_mileage_km: # If \"K km\" format is found, convert it to meters and append to the cleaned list\n",
    "                mileage_clean.append(int(match_mileage_km.group(1)) * 1000)\n",
    "            else:\n",
    "                mileage_clean.append(int(match_mileage_miles.group(1)) * 1600) # If \"K miles\" format is found, convert it to meters and append to the cleaned list\n",
    "                \n",
    "    # Add all values to a list of dictionaries\n",
    "    vehicles_list = []\n",
    "    for i, item in enumerate(titles_list):\n",
    "        cars_dict = {}\n",
    "    \n",
    "        title_split = titles_list[i].split()\n",
    "    \n",
    "        cars_dict[\"Year\"] = int(title_split[0])\n",
    "        cars_dict[\"Make\"] = title_split[1]\n",
    "        cars_dict[\"Model\"] = title_split[2]\n",
    "        cars_dict[\"Price\"] = int(re.sub(r'[^\\d.]', '', prices_list[i]))\n",
    "        cars_dict[\"Mileage\"] = mileage_clean[i]\n",
    "        head, sep, tail = urls_list[i].partition('/?ref') # Shortens URL for comparison\n",
    "        cars_dict[\"URL\"] = head\n",
    "        cars_dict[\"Date\"] = f'{date.today()}'\n",
    "        cars_dict[\"Time\"] = f'{time.strftime(\"%H:%M\", time.localtime())}'\n",
    "        vehicles_list.append(cars_dict)\n",
    "    return vehicles_list\n",
    "url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wipe data file\n",
    "totalDataFile = [{'Year': '#null', 'Make': '#null', 'Model': '#null', 'Price': '#null', 'Mileage': '#null', 'URL': '#null', 'Date and Time': '#null'}] # Null values to start loop, removed after first car is listed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "20\n",
      "20.0\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "# Collect data once\n",
    "recentDataFile = skim() # Execute skimming function\n",
    "\n",
    "tempDataFile = [] # Creates temp list to avoid infinite loop\n",
    "for i, car in enumerate(totalDataFile): # Creates copy of totalDataFile variable\n",
    "    tempDataFile.append(totalDataFile[i])\n",
    "for i, listing in enumerate(recentDataFile): # Checks for duplicate listings using the URL\n",
    "    true = 0\n",
    "    for j, car in enumerate(totalDataFile):\n",
    "        if recentDataFile[i].get('URL') == totalDataFile[j].get('URL') and true == 0:\n",
    "            true = 1\n",
    "        if j+1 == len(totalDataFile) and true == 0:\n",
    "            tempDataFile.append(listing)\n",
    "            print('New Car Listed')\n",
    "            if tempDataFile[0].get('Year') == '#null': # Remove null row if this is first run\n",
    "                tempDataFile.pop(0)\n",
    "                \n",
    "totalDataFile = tempDataFile # Removes need of temporaryDataFile\n",
    "vehicles_df = pd.DataFrame(totalDataFile) # Creates a chart\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "vehicles_df['URL'] = 'https://www.facebook.com/' + vehicles_df['URL'] # Convert to proper URLs\n",
    "vehicles_df.to_csv(csv_file_path, index=False)# Convert and save recent scan to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
